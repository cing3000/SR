{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints/subpixel/\n"
     ]
    }
   ],
   "source": [
    "flags = tf.app.flags\n",
    "\n",
    "flags.DEFINE_string('f', '', 'kernel')          # Weird jupter-notebook 'f' flag\n",
    "\n",
    "flags.DEFINE_string(\"checkpoint_dir\", \"checkpoints/subpixel/\", \"\")\n",
    "flags.DEFINE_string(\"sample_dir\", \"samples/subpixel/\", \"\")\n",
    "flags.DEFINE_integer(\"split_stride\", 60, \"\")\n",
    "flags.DEFINE_integer(\"scale\", 4, \"\")\n",
    "flags.DEFINE_integer(\"image_size\", 64, \"\")\n",
    "flags.DEFINE_integer(\"batch_size\", 128, \"\")\n",
    "flags.DEFINE_integer(\"num_epochs\", 200, \"\")\n",
    "flags.DEFINE_float(\"learning_rate\", 1e-4, \"\")\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "print(FLAGS.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1008 train data have been processed.\n",
      "Total 40 valid data have been processed.\n"
     ]
    }
   ],
   "source": [
    "def _float32_feature(value):\n",
    "    # reshape value from any dimensions to one dimension, which is the only shape FloatList accepts.\n",
    "    value = value.flatten()\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[float(v) for v in value]))\n",
    "\n",
    "#\n",
    "#  Input:\n",
    "#          hr_dataset_dir: HR image directory,\n",
    "#          lr_dataset_dir: LR image directory, should includes exactly the same name and number of files with HR image folder\n",
    "#          target_dir: where it save dataset files\n",
    "#          dataset_type: it could be 'train', 'valid' or 'test', used as prefix of saved files\n",
    "#\n",
    "#  Output:\n",
    "#          None\n",
    "#\n",
    "def preprocess_div2k(hr_dataset_dir, lr_dataset_dir, target_dir, dataset_type='train'):\n",
    "\n",
    "    # Create the first writer\n",
    "    savepath = os.path.join(target_dir, '%s_0.tfrecord' % dataset_type)\n",
    "    writer_options = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.GZIP)\n",
    "    writer = tf.python_io.TFRecordWriter(savepath, options=writer_options)\n",
    "\n",
    "    num_data = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for filename in sorted(os.listdir(hr_dataset_dir)):\n",
    "\n",
    "        # read files\n",
    "        name, ext = filename.split(\".\")\n",
    "        hr_file = os.path.join(hr_dataset_dir, filename)\n",
    "        lr_file = os.path.join(lr_dataset_dir, name+'x'+str(FLAGS.scale)+'.'+ext)\n",
    "        \n",
    "        hr_image = plt.imread(hr_file)\n",
    "        lr_image = plt.imread(lr_file)\n",
    "\n",
    "        # short variables name\n",
    "        h, w, _ = lr_image.shape\n",
    "        ls = FLAGS.image_size\n",
    "        hs = FLAGS.image_size * FLAGS.scale\n",
    "        \n",
    "        # split images\n",
    "        for y in range(0, h-ls+1, FLAGS.split_stride):\n",
    "            hy = y * FLAGS.scale\n",
    "            for x in range(0, w-ls+1, FLAGS.split_stride):\n",
    "                hx = x * FLAGS.scale\n",
    "                \n",
    "                x_i = lr_image[y:y+ls, x:x+ls]\n",
    "                y_i = hr_image[hy:hy+hs, hx:hx+hs]\n",
    "\n",
    "                # save\n",
    "                writer.write(tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'X': _float32_feature(x_i),\n",
    "                    'Y': _float32_feature(y_i)\n",
    "                })).SerializeToString())\n",
    "\n",
    "                num_data += 1\n",
    "                \n",
    "                if num_data % 2000 == 0:\n",
    "\n",
    "                    # flush and close preivous writer\n",
    "                    writer.flush()\n",
    "                    writer.close()\n",
    "                    print(\"%d %s data have been saved, elapsed %.2f sec\" % (num_data, dataset_type, (time.time()-start_time)))\n",
    "                    start_time = time.time()\n",
    "                    \n",
    "                    # create a new file\n",
    "                    savepath = os.path.join(target_dir, '%s_%d.tfrecord' % (dataset_type, (num_data//2000)))\n",
    "                    writer = tf.python_io.TFRecordWriter(savepath, options=writer_options)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    print(\"Total %d %s data have been processed.\" % (num_data, dataset_type))\n",
    "\n",
    "\n",
    "# Train data\n",
    "preprocess_div2k('datasets/DIV2K/DIV2K_train_HR', 'datasets/DIV2K/DIV2K_train_LR',\n",
    "                 FLAGS.checkpoint_dir, dataset_type='train')\n",
    "\n",
    "# Validation data\n",
    "preprocess_div2k('datasets/DIV2K/DIV2K_valid_HR', 'datasets/DIV2K/DIV2K_valid_LR',\n",
    "                 FLAGS.checkpoint_dir, dataset_type='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Parse dataset from tfrecord file\n",
    "#\n",
    "def _parse_function(example_proto):\n",
    "    features = {\"X\": tf.FixedLenFeature([FLAGS.image_size*FLAGS.image_size*3], tf.float32),\n",
    "                \"Y\": tf.FixedLenFeature([FLAGS.image_size*FLAGS.image_size*3*FLAGS.scale*FLAGS.scale], tf.float32)\n",
    "               }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    data = tf.reshape(parsed_features[\"X\"], [FLAGS.image_size, FLAGS.image_size, 3])\n",
    "    labels = tf.reshape(parsed_features[\"Y\"], [FLAGS.image_size*FLAGS.scale, FLAGS.image_size*FLAGS.scale, 3])\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  model/input: (?, 64, 64, 3)\n",
      "[TL] Conv2d model/conv2d: n_filter: 48 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] SubpixelConv2d  model/subpixel_conv2d: scale: 4 n_out_channel: 3 act: No Activation\n",
      "Epoch:  1, time: 2.99, loss: 2241.16608500\n",
      "Epoch: 21, time: 6.51, loss: 224.63528439\n",
      "Epoch: 41, time: 9.23, loss: 104.43742387\n",
      "Epoch: 61, time: 6.62, loss: 74.47006647\n",
      "Epoch: 81, time: 9.21, loss: 59.42241754\n",
      "Epoch: 101, time: 6.47, loss: 59.38655231\n",
      "Epoch: 121, time: 9.38, loss: 45.38040143\n",
      "Epoch: 141, time: 6.40, loss: 47.37161100\n",
      "Epoch: 161, time: 9.50, loss: 44.56048366\n",
      "Epoch: 181, time: 6.41, loss: 43.81539766\n",
      "Epoch: 201, time: 9.09, loss: 40.17355852\n",
      "Epoch: 221, time: 9.03, loss: 45.49504258\n",
      "Epoch: 241, time: 6.26, loss: 40.22292793\n",
      "Epoch: 261, time: 9.11, loss: 40.33448175\n",
      "Epoch: 281, time: 6.32, loss: 37.17411077\n",
      "[TL] InputLayer  model/input: (?, 64, 64, 3)\n",
      "[TL] Conv2d model/conv2d: n_filter: 48 filter_size: (3, 3) strides: (1, 1) pad: SAME act: relu\n",
      "[TL] SubpixelConv2d  model/subpixel_conv2d: scale: 4 n_out_channel: 3 act: No Activation\n",
      "0.0022055123\n",
      "Validation mean error: 0.00\n",
      "(40, 256, 256, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3Xl0nHd97/H3Z2Y0I0vybjkkcYydDRICScAEKEtbEiChIQllSaDcG3rpCS3QplAooQuUwC1QzgVKD4WkkJayBQgFAg2EEBIoS8DOCk5IcBwTK4s3eZNkLTPzvX88j+SxNNKMZY1Gy+d1zpyZZ5v5PrI8X/12RQRmZmYTyTQ7ADMzm/mcLMzMrCYnCzMzq8nJwszManKyMDOzmpwszMysJicLm7UkZSX1SFo9lefOFpJykkLSmnGOXyrpO9Mblc1V8jgLmy6Seio224ABoJRuvzEivjD9UR05Se8HVkXE66f5c3PAELA2IrYcwft8HtgUEf8wRaHZHJRrdgA2f0REx/BrSVuAP4mI7493vqRcRBSnIzabPEnZiCjVPtNmM1dD2Ywh6f2SvizpS5L2A6+T9BxJt0naI+kxSR+X1JKef0g1jKTPp8e/I2m/pJ9JWnu456bHz5P0gKS9kv5F0k8kvX4S9/QUST9M4/+lpD+oOHa+pPvSz++S9NZ0/0pJN6TXdEv6UY2PeYmkTZJ2S/p4xfv/iaRb09eZ9H63p/d0j6RTJb0JuBj4m7Sa7ut1xP15SZ+Q9F1JvcBfS3pUUqbinIslbTjcn5fNXE4WNtO8HPgisBj4MlAELgdWAM8FzgXeOMH1rwX+HlgGPAy873DPlbQS+ArwjvRzHwLOOtwbkZQHvg38N9AJvBX4sqQT01P+HXhDRCwEngb8MN3/DmBzes0T0hgn8lLgGcCZJAn2nCrnnAc8GzgJWApcAnRHxL+S/Jz/MSI6IuLldcQNyc/uvcBC4CPAfuDsiuOvAz5XI26bRZwsbKb5cUR8KyLKEXEgItZHxM8johgRm4Grgd+d4PrrImJDRAwBXwDOmMS55wN3RcQ302MfBXZO4l6eC+SBD0fEUFrl9h2SL2pI2htOlbQwIroj4o6K/ccAqyNiMCJ+OOadD/WBiNibtlvcSvV7HgIWAU8GiIh7I+LxScYN8PWI+Fn67zQA/CdJgkDSCpLE8aUacdss4mRhM83Wyg1JT5b035Iel7QPuJLkr/3xVH4B9gEd4504wbnHVMYRSS+QrjpiH+0Y4OE4tBfJb4Fj09cvBy4AHpZ0q6Rnpfs/mJ53s6QHJb2jxufUvOeI+B7wKeCTwDZJn5K0cJJxw6h/J5JSxEWS2kiSyi0Rsb1G3DaLOFnYTDO6e95VwK+AEyNiEfBuQA2O4TFg1fCGJHHoF2W9HgWOS68fthp4BCAtMV0ArCSp9rk23b8vIt4aEWuAi4B3SpqoNFWXiPhYRDwdOA04FXjb8KHDibvaNRHxMLABuBD4X7gKas5xsrCZbiGwF+iVdAoTt1dMlW8DT5f0srR76uUkdfcTyUpqrXgUgJ+StLn8laQWSS8kaV/4iqQFkl4raVFa1bWftBtx+rknpF/We9P9R9TbSNJZ6SMH9AKDFe+5DTi+4vRx467xMf8JvIukquubRxKvzTxOFjbT/RVwKcmX6VUkjbENFRHbSHoIfQTYBZwA3EkyLmQ8rwMOVDzuT+vyX0by1/ZO4OPAayPigfSaS4HfptVrbyD5ixzgScAPgB7gJ8A/R8SPj/C2lgCfAfYAW0hKTx9Nj30aOD3tTXVdHXGP52skSee6iDhwhPHaDONBeWY1SMqSVM28MiL+p9nxzFRpSegh4PURcWuTw7Ep5pKFWRWSzpW0OK1O+nuSaplfNDmsme7VJKWvWr23bBbyCG6z6p5H0p02D2wELkqrZ6wKST8mGcPxR+HqijnJ1VBmZlaTq6HMzKymOVMNtWLFilizZk2zwzAzm1Vuv/32nRFRq2v43EkWa9asYcMGz1tmZnY4JP22nvNcDWVmZjU5WZiZWU1OFmZmVpOThZmZ1eRkYWZmNTlZmJlZTU4WZmZW07xPFj0DRT5y0wPctXVPs0MxM5ux5n2yGCyW+fjNv+Guh3c3OxQzsxlr3ieLQi75EQyWyk2OxMxs5pr3ySKfJouBIScLM7PxzPtkkcuIjGCg6GRhZjaeeZ8sJFHIZV0NZWY2gXmfLCCpihoYKjU7DDOzGcvJgqSR29VQZmbjc7IACi0ZBp0szMzG5WQBFHJZlyzMzCbgZAHksxkGim6zMDMbj5MFSTWUSxZmZuNzssAN3GZmtThZAHm3WZiZTcjJgrRk4XEWZmbjcrIgSRYewW1mNj4nC4ZHcDtZmJmNx8kCj7MwM6vFyYK0GsrjLMzMxuVkgcdZmJnV0tBkIelcSfdL2iTpiirH3ybpXkn3SLpZ0hMrjl0q6Tfp49JGxlnIJskiIhr5MWZms1bDkoWkLPAJ4DzgVOA1kk4dddqdwLqIeBpwHfBP6bXLgPcAzwLOAt4jaWmjYi20ZAEvrWpmNp5GlizOAjZFxOaIGASuBS6sPCEibomIvnTzNmBV+volwE0R0R0Ru4GbgHMbFejIOtyuijIzq6qRyeJYYGvFdle6bzxvAL5zONdKukzSBkkbduzYMelAR9bhdrIwM6uqkclCVfZVbRSQ9DpgHfDhw7k2Iq6OiHURsa6zs3PSgRacLMzMJtTIZNEFHFexvQp4dPRJks4B/ha4ICIGDufaqVLIpW0WThZmZlU1MlmsB06StFZSHrgEuL7yBElnAleRJIrtFYduBF4saWnasP3idF9DHKyG8lgLM7Nqco1644goSnoLyZd8FrgmIjZKuhLYEBHXk1Q7dQBflQTwcERcEBHdkt5HknAAroyI7kbFOlIN5Sk/zMyqaliyAIiIG4AbRu17d8Xrcya49hrgmsZFd9BINZS7zpqZVeUR3CQjuMElCzOz8ThZkKzBDW6zMDMbj5MFFSUL94YyM6vKyQJ3nTUzq8XJAnedNTOrxckCj+A2M6vFyQJPJGhmVouTBZ5I0MysFicLKrrODrnNwsysGicLQBKFXIYBj+A2M6vKySJVyGU8gtvMbBxOFql8Lus2CzOzcThZpAq5jMdZmJmNw8kiVWjJuOusmdk4nCxS+WzG1VBmZuNwskgVWtxmYWY2HieLVCGXYdBtFmZmVTlZpJIGbpcszMyqcbJIeZyFmdn4nCxShVzWa3CbmY3DySLlcRZmZuNzskjlXQ1lZjYuJ4uUG7jNzMbnZJEqtGQ9gtvMbBxOFqlkBHeJiGh2KGZmM46TRaqQy1AOKJadLMzMRnOySBVavA63mdl4nCxSI0urOlmYmY3hZJEqtGQBPNbCzKwKJ4tUIedqKDOz8ThZpAq54ZKFk4WZ2WhOFql8WrLwKG4zs7GcLFLD1VBuszAzG8vJIuU2CzOz8TlZpEaqoZwszMzGaGiykHSupPslbZJ0RZXjL5B0h6SipFeOOlaSdFf6uL6RcUJlA7eroczMRss16o0lZYFPAC8CuoD1kq6PiHsrTnsYeD3w9ipvcSAizmhUfKMNj+B2ycLMbKyGJQvgLGBTRGwGkHQtcCEwkiwiYkt6rOnf0B7BbWY2vkZWQx0LbK3Y7kr31atV0gZJt0m6aGpDG8slCzOz8TWyZKEq+w5nStfVEfGopOOBH0j6ZUQ8eMgHSJcBlwGsXr168pFysM3CvaHMzMZqZMmiCziuYnsV8Gi9F0fEo+nzZuBW4Mwq51wdEesiYl1nZ+cRBetxFmZm42tkslgPnCRpraQ8cAlQV68mSUslFdLXK4DnUtHW0QgjbRYewW1mNkbDkkVEFIG3ADcC9wFfiYiNkq6UdAGApGdK6gJeBVwlaWN6+SnABkl3A7cAHxzVi2rKZTJKV8tzsjAzG61mm4WkfwLeDxwAvgucDvxlRHy+1rURcQNww6h97654vZ6kemr0dT8Fnlrr/adaIZdxm4WZWRX1lCxeHBH7gPNJ2iFOBt7R0KiaJJ/LuM3CzKyKepJFS/r8UuBLEdHdwHiaqpBzNZSZWTX1dJ39lqRfk1RDvUlSJ9Df2LCao9CSdTWUmVkVNUsWEXEF8BxgXUQMAb0kI7HnnIKroczMqqqZLCS9CihGREnS3wGfB45peGRNkHc1lJlZVfW0Wfx9ROyX9DzgJcBngU82NqzmcG8oM7Pq6kkWw/UyfwB8MiK+CeQbF1LzFHJZlyzMzKqoJ1k8Iukq4NXADenI6jm5aJK7zpqZVVfPl/6rSUZhnxsRe4BlzNFxFoVcxtN9mJlVUU9vqD7gQeAlkt4CrIyI7zU8siYo5DIMlpwszMxGq6c31OXAF4CV6ePzkv680YE1Q94lCzOzquoZlPcG4FkR0Qsg6UPAz4B/aWRgzZA0cLvNwsxstHraLMTBHlGkr6stbDTrueusmVl19ZQs/h34uaSvp9sXAZ9pXEjNU2jxoDwzs2pqJouI+IikW4HnkZQo/jgi7mx0YM2Qz2YploNSOchm5mThycxsUsZNFpKWVWxuSR8jx+bi7LOFlqRWbrBYZkE+2+RozMxmjolKFrcDwcH2iUiflb4+voFxNUXlOtxOFmZmB42bLCJi7XQGMhPkR5KF2y3MzCrNyWk7JquQS0oTHmthZnYoJ4sKw9VQgyWPtTAzq+RkUWG4GqrfJQszs0PU7Do7qlfUsP3pqnlzSsFtFmZmVdVTsrgD2AE8APwmff2QpDskPaORwU234TYLj+I2MztUPcniu8BLI2JFRCwHzgO+ArwJ+NdGBjfdhsdZeH4oM7ND1ZMs1kXEjcMb6fTkL4iI24BCwyJrgnzW1VBmZtXUMzdUt6R3Atem2xcDuyVlgTn1rdpaMYLbzMwOqqdk8VpgFfAN4JvA6nRflmQVvTljZJyFk4WZ2SHqmUhwJzDeYkebpjac5srn3GZhZlZNPV1nTwbeDqypPD8iXti4sJpjpOusx1mYmR2injaLrwKfAj7NoYsgzTkjXWe9DreZ2SHqSRbFiPhkwyOZAfIuWZiZVVVPA/e3JL1J0tGSlg0/Gh5ZE2QzIpeR2yzMzEapp2Rxafr8jop9c3I9C/A63GZm1dTTG2perWtRaMm666yZ2SgTLav6woj4gaQ/rHY8Iv6rcWE1Tz6bcTWUmdkoE5Usfhf4AfCyKscCmJPJotDiaigzs9EmWlb1PenzH0/2zSWdC/wzyWjvT0fEB0cdfwHwMeBpwCURcV3FsUuBv0s33x8Rn51sHIejkMu4GsrMbJR6BuUVgFcwdlDelTWuywKfAF4EdAHrJV0fEfdWnPYw8HqSQX+V1y4D3gOsIynF3J5eu7v2LR2ZvJOFmdkY9XSd/SZwIVAEeisetZwFbIqIzRExSDIR4YWVJ0TEloi4h7ETEr4EuCkiutMEcRNwbh2fecQKuazbLMzMRqmn6+yqiJjMF/WxwNaK7S7gWUdw7bGjT5J0GXAZwOrVqycR4ljuOmtmNlY9JYufSnrqJN5bVfbFVF4bEVdHxLqIWNfZ2XlYwY3H1VBmZmPVkyyeR9JmcL+keyT9UtI9dVzXBRxXsb0KeLTOuI7k2iNSyGU83YeZ2Sj1VEOdN8n3Xg+cJGkt8AhwCck6GPW4EfhHSUvT7RcD75pkHIelkMt6IkEzs1HGLVlIWpS+3D/OY0IRUQTeQvLFfx/wlYjYKOlKSRekn/FMSV3Aq4CrJG1Mr+0G3keScNYDV6b7Gi4pWbiB28ys0kQliy8C5wO3k7QXVLYj1DU3VETcANwwat+7K16vJ6liqnbtNcA1tT5jqrnNwsxsrIkG5Z2fPs+vuaFyWfeGMjMbpZ42C9K2g5OA1uF9EfGjRgXVTIUWlyzMzEarZwT3nwCXk1QX3QU8G/gZMOeWVYVkIsHBUplyOchkqvXgNTObf+rpOns58EzgtxHx+8CZwI6GRtVEhZbkR+IeUWZmB9WTLPojoh+SeaIi4tfAkxobVvMMr8Ptqigzs4PqabPokrQE+AZwk6TdTNMAuWYYWYe7WAJamhuMmdkMUc9KeS9PX/6DpFuAxcB3GxpVExWGk4VHcZuZjZgwWUjKAPdExGkAEfHDaYmqiYaThdsszMwOmrDNIiLKwN2SpmZK11lgpM3CJQszsxH1tFkcDWyU9Asq1rGIiAsaFlUTFQ5pszAzM6gvWby34VHMICPVUO4NZWY2op5k8dKIeGflDkkfAuZk+8XwOAt3nTUzO6iecRYvqrJvstOWz3j5rMdZmJmNNm7JQtKfAW8Cjh+12NFC4CeNDqxZDpYs3GZhZjas1hTl3wE+AFxRsX//dK0t0QxuszAzG2uiKcr3AnuB10xfOM13cAS3k4WZ2bB62izmlYPjLFwNZWY2zMliFI/gNjMby8liFM8NZWY2lpPFKLlshozcZmFmVsnJoopCLutqKDOzCk4WVRRaMm7gNjOr4GRRRT6bcTWUmVkFJ4sqCi1OFmZmlZwsqijksh7BbWZWwcmiiqQaym0WZmbDnCyqcDWUmdmhnCyqKOScLMzMKjlZVFHIZZ0szMwqOFlUkc95nIWZWSUniyoKuYxHcJuZVXCyqKKQy3oiQTOzCk4WVeTdwG1mdggniyqS3lBuszAzG+ZkUUWhJeMR3GZmFZwsqiikEwlGRLNDMTObERqaLCSdK+l+SZskXVHleEHSl9PjP5e0Jt2/RtIBSXelj081Ms7RCi3JOtzuEWVmlsg16o0lZYFPAC8CuoD1kq6PiHsrTnsDsDsiTpR0CfAh4OL02IMRcUaj4pvIyDrcxTKFXLYZIZiZzSiNLFmcBWyKiM0RMQhcC1w46pwLgc+mr68DzpakBsZUl5F1uN1uYWYGNDZZHAtsrdjuSvdVPSciisBeYHl6bK2kOyX9UNLzq32ApMskbZC0YceOHVMWeN7JwszsEI1MFtVKCKNbjMc75zFgdUScCbwN+KKkRWNOjLg6ItZFxLrOzs4jDnjYcNWTe0SZmSUamSy6gOMqtlcBj453jqQcsBjojoiBiNgFEBG3Aw8CJzcw1kMcrIbyWAszM2hsslgPnCRpraQ8cAlw/ahzrgcuTV+/EvhBRISkzrSBHEnHAycBmxsY6yFGqqE85YeZGdDA3lARUZT0FuBGIAtcExEbJV0JbIiI64HPAJ+TtAnoJkkoAC8ArpRUBErAn0ZEd6NiHW24GsptFmZmiYYlC4CIuAG4YdS+d1e87gdeVeW6rwFfa2RsEym0HOw6a2ZmHsFdVT7rNgszs0pOFlUMlyxcDWVmlnCyqGK8rrN3PrybjY/ubUZIZmZN5WRRRbWus/c9to/X/NttvPXLdzUrLDOzpnGyqGL0CO69fUO88XO30z9U5oFtPWzt7mtmeGZm087JoorKiQRL5eDyL9/JY3sP8NGLTwfg+/dta2Z4ZmbTzsmiispxFh/7/gPcev8O3vOyp/DyM1dxQmc7N9+3vckRmplNr4aOs5itWrLJlFXfu3cbd2/dw6vXreKPnrUagHNOOYrP/Pgh9vUPsai1pZlhmplNG5csqpBEIZfh7q17OH3VYq688DSGZ04/+5SjKJaDHz0wdbPcmpnNdE4W42htybK8Pc8nX/cMWlsOLoD09NVLWNLW4qooM5tXXA01jvde8BROXNnBMUsWHLI/l83w+09ayS33b6dYKpPLOt+a2dznb7pxXHTmsZx27OKqx8455Sj29A1xx8N7pjkqM7PmcLKYhBecvIKWrLjZXWjNbJ5wspiEha0tPGvtco+3MLN5w8liks4+ZSUP7uhly87eZodiZtZwThaTdM4pRwEezW1m84OTxSQdt6yNk4/qcBdaM5sXnCyOwNmnHMUvtnSzt2/okP3FUpnu3sEmRWVmNvWcLI7AOaespFQObn1gOxHB3Vv38N5vbeTZH7iZ53zgZu5/fH+zQzQzmxKKiGbHMCXWrVsXGzZsmNbPLJWDZ/7f79PZUWCoVGbzzl7yuQxnP3klv3iom2OXLuC//ux3PHDPzGYsSbdHxLpa5/lb7AhkM+K8057AA9v3s3JRgQ+94qms/9tz+OTrnsH7LjqNe7r2ctWPNjc7TDOzI+aSxREaKJboHSixrD0/5tibv3AHN927jW//xfM4+aiF0x6bmVktLllMk0IuWzVRALz3wqfQ0ZrjHV+9m2KpXPUcM7PZwMmigVZ0FLjywqdwd9de/u1/Hmp2OGZmk+Zk0WB/8NSjOe+0J/DRmx7gN9vcO8rMZidPUd5gkrjywtO4bfMPedtX7uZ1z15NRiKXFRmJQi7L809aQXvB/xRmNnP5G2oadC4s8P6Lnsrl197JO7/2yzHHVy4s8PaXPIlXPH0V2YyaEKGZ2cTcG2oa7e0bonewSKkcFMtBqRw8tvcA/+97D3DX1j2cevQi/u78U/idE1Y0O1Qzmyfq7Q3lZDEDRATfuucxPvSdX/PIngO86NSjeP5JYxNGWz7H8vY8yzvyLO8osLw9f8iSr2Zmh6veZOFqqBlAEhecfgwvPvUorvnJQ/zrLQ9y0731zWZ73LIFvOLpq3jlM1axamlbgyM1s/nKJYsZqH+oRO9A8ZB9AfQOFNnVO8iunkG6ewfY2TPIbZt38eNNOwF47gkreNW6Vbzo1GT69IGhMgPFMgPFEqVysKQtz5IFLWTcLmJmKVdDzSNdu/v42u2P8NXbt9K1+8CE52YEy9rzI4/Oha10dhToXHjwsbziuKu5zOY2J4t5qFwOfvrgLu58eDf5XIZCLkOhJUs+myGTgT19Q+zqGWRXb1Iy2dUzyM6eAXbsH6B3sFT1PdvyWZa25VmxsMDKkUcrKxcV6CjkKEdQjqBUTj4/kxEdhRyLWnMsbG1hYWuOjMTmnT1s2t7Db7b1sGlHD127+1jRUeCJy9s4blkbT1zWznHLFpCVGCiVGRgqM1gqMzBUIp/L0J7P0V7I0VHI0V7IUmjJkhFkJERSlVdoybCwkENyycmsXk4Wdlh6B4rs7Blg+/4BunsHRx670+cdaVIZPj5Zy9vznLCyg+OWtrGzZ4Ct3X1s3d3HUGlqfg+zGbF4QQtL2lpY2pZnaVueoxe3cvSS1uR58QKWtuV5ZE8fD+3s46GdPWzZ2cfD3X0EQUs2Qz6bIZ/L0JLNsHhBS9qpoMCKjjwrOgosyFcvbQ0nreQZMhklCTuXTZ+T9x1+73wu+ayWbIaIoBRBuQylSHrK9Q0W2d8//Bhif3+RgWI5SdDl9Pz0x5bLiGxGI8+FXJZFC3IsXtAy8uiY4Ym0XA627e9nyYL8uD/jYYPFMhnhGZ2ngBu47bC0F5K/3J+4vL3muYPFMjt6BugbKJLJiKySLygJymXYl36xDX/BFctl1ixv58SVHSzvKIx5v1I5eHxfP13dfQSMfMEOl44GS2V6B4r0DBTpHSjRMzDEYLFMBJQDyhFEBAPFMnv6htjdN8ieA0Ps6Ruka3cf67d0s/fA0NgbARa15ljb2cEZxy0hl0lKNUPFpFQzWCyzbV8/9z66j129A1OW0JppOJlkK5JLW/5gia2jtYWOQpZCLktLVgcTWzZDOYLB9GczUEx+PvlsJqmy7Mizor3AsvY8uaxG/rDYkT72DxRZsqCFZe15VnTkWdZeoL2Q5be7+ti0PSl1bt7ZQ/9QMofaUYsKPHFZO09c3sbqZW30F0t07T5A1+4DPLL7ANv295OVWL28jeNXdHBCZzvHd7azeEGePX2D7O5L/v27ewcZLJXp7CjwhMWtHLVo+FFgUWsLHa05WmoknN6BIg/t7GXLrl4e2tHLll19SHDskgXJY+kCjlmygCULWtIEfjDpR8TIHxAZpf9HIujpL7Kv4v9I32CR1pYsC1tzdBSSEnl7PsfeA0Ns29fPtv39bNs3wPZ9/UhiaVvys1zSlmdpWwtPWNzKU45Z3NDfHZcsbF7oGyzy2N5+Ht/bT3fvIMcsWcDaFe0sbWup66/tiGBff1L66h8aW2U3/N8oAoIgIvmyGCymnQyGSmlngzJDpeQx/MU7VIykSi2TjOrPZpIvlrZ8joWtw4/kC6Q1l0Vi5Ms+IxEkX05DpfLIGJ7+oRL7+ofYd2CIvemjp7+YjO+JoFRKnouloHewOJKMewZK9PQPjcQ1VDqYODPSSMkon00S+UCxzK7egZEv+dEWFnJ0LiywsDXHngNDdPcMsn9U541jlyzgxJUdnLiygzUr2tnbN8iWXX08vKuP33b3sm3fALmMOHpJa/oF3caxSxcka8js6Em+yHf2MThqss5CLsPStjz5XIbt+/vHjbG1JcPC1qTkFZH8/Iql5HmoVB7zh8YTFrUSBNv3DzCdX59SUjKPgD0HhiiVD374Gcct4Rtvfu4k33cGlCwknQv8M5AFPh0RHxx1vAD8J/AMYBdwcURsSY+9C3gDUAL+IiJubGSsNre15XOc0NnBCZ0dk7pe0kh1jo3VN1hMe+kNMlQqj3SWaMuP/YoZKJbY3TvE/v4hjl26oOo5lfqHSrRkMxPOblAqB4/sPsC+/iGWtudZ1nZoVVZEsO9AkW37kz8Ytu8fGPmrfvi5Z6CIJFoyyXQ82UyGlqw4alEra1e0s2Z5O2tWtI3EO1gs8/jefh7Zc4BH9hxgf//QSAIfTvpCI388BEmpQoiO9I+A4ba9tnyW/qFykrD7i/QMJDEtWtAyUhJa0VEYKQWVy8H+geJIKWo6KhcbVrKQlAUeAF4EdAHrgddExL0V57wJeFpE/KmkS4CXR8TFkk4FvgScBRwDfB84OSKqt8LikoWZ2WTMhPUszgI2RcTmiBgErgUuHHXOhcBn09fXAWcrqRO4ELg2IgYi4iFgU/p+ZmbWBI1MFscCWyu2u9J9Vc+JiCKwF1he57VIukzSBkkbduzYMYWhm5lZpUYmi2rVaKPrvMY7p55riYirI2JdRKzr7OycRIhmZlaPRiaLLuC4iu1VwKPjnSMpBywGuuu81szMpkkjk8V64CRJayXlgUuA60edcz1wafr6lcAPImlxvx64RFJB0lrgJOAXDYzVzMwm0LCusxFRlPQW4EaSrrPXRMRGSVcCGyLieuAzwOckbSIpUVySXrtR0leAe4Ei8OaJekKZmVljeVCemdk8NhO6zpqZ2RwxZ0oWknYAvz2Ct1gB7JyicJptLt2B+3bbAAAFuElEQVQLzK37mUv3Ar6fmazee3liRNTsTjpnksWRkrShnqLYbDCX7gXm1v3MpXsB389MNtX34mooMzOrycnCzMxqcrI46OpmBzCF5tK9wNy6n7l0L+D7mcmm9F7cZmFmZjW5ZGFmZjU5WZiZWU3zPllIOlfS/ZI2Sbqi2fEcLknXSNou6VcV+5ZJuknSb9Lnpc2MsV6SjpN0i6T7JG2UdHm6f7beT6ukX0i6O72f96b710r6eXo/X07nTpsVJGUl3Snp2+n2bL6XLZJ+KekuSRvSfbPydw1A0hJJ10n6dfp/6DlTeT/zOlmkq/l9AjgPOBV4TbpK32zyH8C5o/ZdAdwcEScBN6fbs0ER+KuIOAV4NvDm9N9jtt7PAPDCiDgdOAM4V9KzgQ8BH03vZzfJ8sGzxeXAfRXbs/leAH4/Is6oGI8wW3/XIFnC+rsR8WTgdJJ/p6m7n4iYtw/gOcCNFdvvAt7V7LgmcR9rgF9VbN8PHJ2+Phq4v9kxTvK+vkmyLO+svx+gDbgDeBbJqNpcuv+Q38GZ/CBZKuBm4IXAt0nWnZmV95LGuwVYMWrfrPxdAxYBD5F2WmrE/czrkgV1rsg3Cx0VEY8BpM8rmxzPYZO0BjgT+Dmz+H7Sapu7gO3ATcCDwJ5IVoaE2fU79zHgr4Fyur2c2XsvkCyo9j1Jt0u6LN03W3/Xjgd2AP+eVhN+WlI7U3g/8z1Z1LUin00vSR3A14C/jIh9zY7nSEREKSLOIPmr/CzglGqnTW9Uh0/S+cD2iLi9cneVU2f8vVR4bkQ8naQa+s2SXtDsgI5ADng68MmIOBPoZYqr0OZ7spirK/Jtk3Q0QPq8vcnx1E1SC0mi+EJE/Fe6e9bez7CI2APcStIWsyRdGRJmz+/cc4ELJG0BriWpivoYs/NeAIiIR9Pn7cDXSZL5bP1d6wK6IuLn6fZ1JMljyu5nvieLelbzm40qVyC8lKTuf8aTJJIFse6LiI9UHJqt99MpaUn6egFwDkmj4y0kK0PCLLmfiHhXRKyKiDUk/09+EBF/xCy8FwBJ7ZIWDr8GXgz8iln6uxYRjwNbJT0p3XU2yeJxU3c/zW6YafYDeCnwAEld8t82O55JxP8l4DFgiOSvizeQ1CXfDPwmfV7W7DjrvJfnkVRj3APclT5eOovv52nAnen9/Ap4d7r/eJJlgjcBXwUKzY71MO/r94Bvz+Z7SeO+O31sHP6/P1t/19LYzwA2pL9v3wCWTuX9eLoPMzOrab5XQ5mZWR2cLMzMrCYnCzMzq8nJwszManKyMDOzmpwszKqQ9NP0eY2k107xe/9Ntc8ym8ncddZsApJ+D3h7RJx/GNdkI6I0wfGeiOiYivjMpotLFmZVSOpJX34QeH665sFb04kBPyxpvaR7JL0xPf/30rU4vgj8Mt33jXSSuo3DE9VJ+iCwIH2/L1R+lhIflvSrdJ2Fiyve+9aKtQq+kI52N5s2udqnmM1rV1BRski/9PdGxDMlFYCfSPpeeu5ZwGkR8VC6/X8iojud6mO9pK9FxBWS3hLJ5IKj/SHJKNzTgRXpNT9Kj50JPIVk7qWfkMzV9OOpv12z6lyyMDs8Lwb+dzrt+M9JplM4KT32i4pEAfAXku4GbiOZsPIkJvY84EuRzFS7Dfgh8MyK9+6KiDLJNChrpuRuzOrkkoXZ4RHw5xFx4yE7k7aN3lHb5wDPiYg+SbcCrXW893gGKl6X8P9dm2YuWZhNbD+wsGL7RuDP0qnUkXRyOmvpaIuB3WmieDLJ1OTDhoavH+VHwMVpu0gn8AKSSfrMms5/nZhN7B6gmFYn/QfJOsdrgDvSRuYdwEVVrvsu8KeS7iFZ2vK2imNXA/dIuiOSab6HfZ1kadK7SWbf/euIeDxNNmZN5a6zZmZWk6uhzMysJicLMzOrycnCzMxqcrIwM7OanCzMzKwmJwszM6vJycLMzGr6/35IWaaU5/z4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "num_epochs = 300\n",
    "\n",
    "def model(X, Y, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse) as vs:\n",
    "        n = tl.layers.InputLayer(X, name='input')\n",
    "        n = tl.layers.Conv2d(n, FLAGS.scale*FLAGS.scale*3, act=tf.nn.relu, use_cudnn_on_gpu=True)\n",
    "        n = tl.layers.SubpixelConv2d(n, scale=FLAGS.scale, name='subpixel_conv2d')\n",
    "        Y_hat = n.outputs\n",
    "\n",
    "        loss = tf.reduce_mean(tf.square(Y - Y_hat))\n",
    "        return Y_hat, loss\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # load train dataset\n",
    "    train_filepath_list = os.path.join(FLAGS.checkpoint_dir, \"train_*.tfrecord\")\n",
    "    train_file_list = glob.glob(train_filepath_list)\n",
    "\n",
    "    # train\n",
    "    dataset = tf.data.TFRecordDataset(train_file_list, compression_type=\"GZIP\")\n",
    "    dataset = dataset.map(map_func=_parse_function, num_parallel_calls=4)\n",
    "    dataset = dataset.shuffle(buffer_size=5000)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(FLAGS.batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=FLAGS.batch_size)\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "        \n",
    "    # pass training data to model\n",
    "    batch_images, batch_labels = iterator.get_next()\n",
    "    \n",
    "    # model\n",
    "    y_hat, loss = model(batch_images, batch_labels)\n",
    "    \n",
    "    # Set learning rate decay\n",
    "    step = tf.Variable(0, trainable=False)\n",
    "    rate = tf.train.exponential_decay(lr, step, 1, 0.9997)\n",
    "    \n",
    "    # Use Adam gradient descent\n",
    "    optimizer = tf.train.AdamOptimizer(rate).minimize(loss, global_step=step)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(iterator.initializer)\n",
    "\n",
    "    loss_hist = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        _, _, err = sess.run((optimizer, y_hat, loss))\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            loss_hist.append(err)\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"Epoch: %2d, time: %.2f, loss: %.8f\" % ((i+1), (time.time()-start_time), (err*10000)))\n",
    "            start_time = time.time()\n",
    "\n",
    "    # load validate dataset\n",
    "    val_filepath_list = os.path.join(FLAGS.checkpoint_dir, \"valid_*.tfrecord\")\n",
    "    val_file_list = glob.glob(val_filepath_list)\n",
    "\n",
    "    # validate\n",
    "    dataset_val = tf.data.TFRecordDataset(val_file_list, compression_type=\"GZIP\")\n",
    "    dataset_val = dataset_val.map(map_func=_parse_function, num_parallel_calls=4)\n",
    "    dataset_val = dataset_val.batch(FLAGS.batch_size)\n",
    "    iterator_val = dataset_val.make_initializable_iterator()\n",
    "    \n",
    "    val_images, val_labels = iterator_val.get_next()\n",
    "\n",
    "    y_hat, loss = model(val_images, val_labels, True)\n",
    "    \n",
    "    sess.run(iterator_val.initializer)\n",
    "    t = 0.0\n",
    "    c = 0\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            predict, err = sess.run((y_hat, loss))\n",
    "            print(err)\n",
    "            t += err*1e+4\n",
    "            c += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            break\n",
    "    \n",
    "    print(\"Validation mean error: %.2f\" % (t*1e+4/c))\n",
    "    print(predict.shape)\n",
    "\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('training loss')\n",
    "plt.title('Training Loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_image(images, size):\n",
    "    h, w = images.shape[1], images.shape[2]\n",
    "    img = np.zeros((size[0], size[1], 3))\n",
    "    s = FLAGS.split_stride * FLAGS.scale\n",
    "    num_cols = 1+(size[1]-w+1)//s\n",
    "    for idx, image in enumerate(images):\n",
    "        i = idx % num_cols\n",
    "        j = idx // num_cols\n",
    "        #print(\"rows: %d : %d, cols: %d : %d\" % (j*s, j*s+h, i*s, i*s+w))\n",
    "        img[j*s+2:j*s+h-2, i*s+2:i*s+w-2, :] = image[2:-2, 2:-2, :]\n",
    "\n",
    "    return img\n",
    "\n",
    "#plt.imsave(\"example.png\", predict[20, :, :, :])\n",
    "\n",
    "merged_y_hat = merge_image(predict, (1356, 2040))\n",
    "merged_y_hat[merged_y_hat>1]=0.9999\n",
    "merged_y_hat[merged_y_hat<0]=0.0\n",
    "ground_truth_file = plt.imread('datasets/DIV2K/DIV2K_valid_HR/0801.png')\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "plt.subplot(211)\n",
    "plt.title('Predict')\n",
    "plt.imshow(merged_y_hat)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.title('Ground Truth')\n",
    "plt.imshow(ground_truth_file)\n",
    "\n",
    "filename = os.path.join(FLAGS.sample_dir, '0801.png')\n",
    "plt.imsave(filename, merged_y_hat)\n",
    "\n",
    "filename = os.path.join(FLAGS.sample_dir, '0801_block.png')\n",
    "plt.imsave(filename, predict[20, :, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - ETA: 2:25 - loss: 8.813 - ETA: 46s - loss: 8.703 - ETA: 33s - loss: 8.64 - ETA: 21s - loss: 8.50 - ETA: 17s - loss: 8.43 - ETA: 14s - loss: 8.35 - ETA: 11s - loss: 8.20 - ETA: 9s - loss: 8.1208 - ETA: 7s - loss: 7.952 - ETA: 6s - loss: 7.865 - ETA: 5s - loss: 7.688 - ETA: 4s - loss: 7.598 - ETA: 3s - loss: 7.417 - ETA: 2s - loss: 7.235 - ETA: 2s - loss: 7.144 - ETA: 1s - loss: 6.964 - ETA: 1s - loss: 6.874 - ETA: 0s - loss: 6.698 - ETA: 0s - loss: 6.612 - ETA: 0s - loss: 6.443 - ETA: 0s - loss: 6.361 - 6s 6ms/step - loss: 6.3184\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 3.700 - ETA: 1s - loss: 3.620 - ETA: 1s - loss: 3.583 - ETA: 1s - loss: 3.549 - ETA: 1s - loss: 3.516 - ETA: 1s - loss: 3.487 - ETA: 1s - loss: 3.434 - ETA: 1s - loss: 3.411 - ETA: 0s - loss: 3.371 - ETA: 0s - loss: 3.354 - ETA: 0s - loss: 3.338 - ETA: 0s - loss: 3.311 - ETA: 0s - loss: 3.299 - ETA: 0s - loss: 3.288 - ETA: 0s - loss: 3.270 - ETA: 0s - loss: 3.263 - ETA: 0s - loss: 3.256 - ETA: 0s - loss: 3.249 - ETA: 0s - loss: 3.238 - ETA: 0s - loss: 3.233 - ETA: 0s - loss: 3.224 - ETA: 0s - loss: 3.220 - ETA: 0s - loss: 3.217 - 2s 2ms/step - loss: 3.1941\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 3.106 - ETA: 1s - loss: 3.103 - ETA: 1s - loss: 3.101 - ETA: 1s - loss: 3.099 - ETA: 1s - loss: 3.098 - ETA: 1s - loss: 3.095 - ETA: 1s - loss: 3.094 - ETA: 0s - loss: 3.092 - ETA: 0s - loss: 3.091 - ETA: 0s - loss: 3.088 - ETA: 0s - loss: 3.087 - ETA: 0s - loss: 3.085 - ETA: 0s - loss: 3.083 - ETA: 0s - loss: 3.082 - ETA: 0s - loss: 3.081 - ETA: 0s - loss: 3.080 - ETA: 0s - loss: 3.078 - ETA: 0s - loss: 3.078 - ETA: 0s - loss: 3.076 - ETA: 0s - loss: 3.075 - ETA: 0s - loss: 3.075 - 2s 2ms/step - loss: 3.0567\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 3.050 - ETA: 1s - loss: 3.052 - ETA: 1s - loss: 3.051 - ETA: 1s - loss: 3.051 - ETA: 1s - loss: 3.051 - ETA: 1s - loss: 3.050 - ETA: 0s - loss: 3.049 - ETA: 0s - loss: 3.049 - ETA: 0s - loss: 3.048 - ETA: 0s - loss: 3.047 - ETA: 0s - loss: 3.047 - ETA: 0s - loss: 3.046 - ETA: 0s - loss: 3.045 - ETA: 0s - loss: 3.045 - ETA: 0s - loss: 3.044 - ETA: 0s - loss: 3.044 - ETA: 0s - loss: 3.043 - ETA: 0s - loss: 3.042 - ETA: 0s - loss: 3.041 - ETA: 0s - loss: 3.041 - 1s 1ms/step - loss: 3.0226\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 3.027 - ETA: 1s - loss: 3.026 - ETA: 1s - loss: 3.025 - ETA: 1s - loss: 3.024 - ETA: 1s - loss: 3.023 - ETA: 1s - loss: 3.022 - ETA: 0s - loss: 3.021 - ETA: 0s - loss: 3.021 - ETA: 0s - loss: 3.020 - ETA: 0s - loss: 3.019 - ETA: 0s - loss: 3.018 - ETA: 0s - loss: 3.018 - ETA: 0s - loss: 3.018 - ETA: 0s - loss: 3.017 - ETA: 0s - loss: 3.016 - ETA: 0s - loss: 3.016 - ETA: 0s - loss: 3.015 - ETA: 0s - loss: 3.014 - ETA: 0s - loss: 3.013 - ETA: 0s - loss: 3.012 - ETA: 0s - loss: 3.012 - 2s 2ms/step - loss: 2.9940\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 2.998 - ETA: 1s - loss: 2.996 - ETA: 1s - loss: 2.995 - ETA: 1s - loss: 2.994 - ETA: 1s - loss: 2.994 - ETA: 1s - loss: 2.993 - ETA: 0s - loss: 2.992 - ETA: 0s - loss: 2.991 - ETA: 0s - loss: 2.990 - ETA: 0s - loss: 2.990 - ETA: 0s - loss: 2.989 - ETA: 0s - loss: 2.988 - ETA: 0s - loss: 2.988 - ETA: 0s - loss: 2.987 - ETA: 0s - loss: 2.987 - ETA: 0s - loss: 2.986 - ETA: 0s - loss: 2.985 - ETA: 0s - loss: 2.984 - ETA: 0s - loss: 2.984 - 2s 2ms/step - loss: 2.9661\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 2.969 - ETA: 1s - loss: 2.969 - ETA: 1s - loss: 2.967 - ETA: 1s - loss: 2.967 - ETA: 1s - loss: 2.966 - ETA: 0s - loss: 2.966 - ETA: 0s - loss: 2.965 - ETA: 0s - loss: 2.964 - ETA: 0s - loss: 2.963 - ETA: 0s - loss: 2.962 - ETA: 0s - loss: 2.962 - ETA: 0s - loss: 2.961 - ETA: 0s - loss: 2.961 - ETA: 0s - loss: 2.960 - ETA: 0s - loss: 2.960 - ETA: 0s - loss: 2.959 - ETA: 0s - loss: 2.958 - ETA: 0s - loss: 2.958 - ETA: 0s - loss: 2.957 - 1s 1ms/step - loss: 2.9393\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 2.940 - ETA: 1s - loss: 2.941 - ETA: 1s - loss: 2.941 - ETA: 1s - loss: 2.940 - ETA: 1s - loss: 2.940 - ETA: 1s - loss: 2.939 - ETA: 0s - loss: 2.939 - ETA: 0s - loss: 2.938 - ETA: 0s - loss: 2.938 - ETA: 0s - loss: 2.937 - ETA: 0s - loss: 2.937 - ETA: 0s - loss: 2.936 - ETA: 0s - loss: 2.935 - ETA: 0s - loss: 2.935 - ETA: 0s - loss: 2.934 - ETA: 0s - loss: 2.934 - ETA: 0s - loss: 2.933 - ETA: 0s - loss: 2.933 - ETA: 0s - loss: 2.932 - ETA: 0s - loss: 2.932 - 1s 1ms/step - loss: 2.9140\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 2.919 - ETA: 1s - loss: 2.918 - ETA: 1s - loss: 2.917 - ETA: 1s - loss: 2.916 - ETA: 1s - loss: 2.916 - ETA: 1s - loss: 2.915 - ETA: 1s - loss: 2.915 - ETA: 0s - loss: 2.914 - ETA: 0s - loss: 2.914 - ETA: 0s - loss: 2.913 - ETA: 0s - loss: 2.913 - ETA: 0s - loss: 2.912 - ETA: 0s - loss: 2.912 - ETA: 0s - loss: 2.911 - ETA: 0s - loss: 2.910 - ETA: 0s - loss: 2.910 - ETA: 0s - loss: 2.909 - ETA: 0s - loss: 2.909 - ETA: 0s - loss: 2.909 - ETA: 0s - loss: 2.908 - 2s 2ms/step - loss: 2.8906\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - ETA: 1s - loss: 2.894 - ETA: 1s - loss: 2.895 - ETA: 1s - loss: 2.894 - ETA: 1s - loss: 2.894 - ETA: 1s - loss: 2.893 - ETA: 1s - loss: 2.893 - ETA: 0s - loss: 2.893 - ETA: 0s - loss: 2.892 - ETA: 0s - loss: 2.892 - ETA: 0s - loss: 2.891 - ETA: 0s - loss: 2.891 - ETA: 0s - loss: 2.890 - ETA: 0s - loss: 2.890 - ETA: 0s - loss: 2.889 - ETA: 0s - loss: 2.888 - ETA: 0s - loss: 2.888 - ETA: 0s - loss: 2.888 - ETA: 0s - loss: 2.887 - ETA: 0s - loss: 2.886 - ETA: 0s - loss: 2.886 - 1s 1ms/step - loss: 2.8692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17011f922b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "from models.subpixel import SubpixelConv2D\n",
    "\n",
    "inputs = keras.layers.Input(shape=(FLAGS.image_size, FLAGS.image_size, 3), name='in')\n",
    "n = keras.layers.Conv2D(FLAGS.scale*FLAGS.scale*3, (3,3), activation='relu', padding='same')(inputs)\n",
    "outputs = SubpixelConv2D(scale=FLAGS.scale)(n)\n",
    "\n",
    "model = keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "data = np.random.random((1000, FLAGS.image_size, FLAGS.image_size, 3))\n",
    "labels = np.random.random((1000, FLAGS.image_size*FLAGS.scale, FLAGS.image_size*FLAGS.scale, 3))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "in (InputLayer)              (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 48)        1344      \n",
      "_________________________________________________________________\n",
      "subpixel_conv2d_5 (SubpixelC (None, 256, 256, 3)       0         \n",
      "=================================================================\n",
      "Total params: 1,344\n",
      "Trainable params: 1,344\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
